{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up IMC-Denoise\n",
    "\n",
    "1. Follow the instructions under the **'Installation'** header from here: https://github.com/PENGLU-WashU/IMC_Denoise. In brief, you need to setup a new conda environment and install some packages with specific version numbers, and then clone and install the IMCDenoise package from Github.\n",
    "\n",
    "2. Run the following command in Anaconda prompt to install a couple of extra packages we will need in the new environment: **conda install tqdm pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and functions\n",
    "Run this to import all the relevant packages and functions. If there is an error here, you have not setup the environment properly. It may also be possible that your graphics card / GPU is not compatible with the script. It *should* all still run, but without acceleration from the GPU, it could be *incredibly* slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, abspath, exists\n",
    "from glob import glob\n",
    "import tifffile as tp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tp\n",
    "from IMC_Denoise.IMC_Denoise_main.DIMR import DIMR\n",
    "from IMC_Denoise.IMC_Denoise_main.DeepSNF import DeepSNF\n",
    "from IMC_Denoise.DeepSNF_utils.DeepSNF_DataGenerator import DeepSNF_DataGenerator\n",
    "\n",
    "### These are adapted from functions from IMC_Denoise\n",
    "\n",
    "def load_single_img(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loading single image from directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : The image file name, must end with .tiff.\n",
    "        DESCRIPTION.\n",
    "    Returns\n",
    "    -------\n",
    "    Img_in : int or float\n",
    "        Loaded image data.\n",
    "    \"\"\"\n",
    "    if filename.endswith('.tiff') or filename.endswith('.tif'):\n",
    "        Img_in = tp.imread(filename).astype('float32')\n",
    "    else:\n",
    "        raise ValueError('Raw file should end with tiff or tif!')\n",
    "    if Img_in.ndim != 2:\n",
    "        raise ValueError('Single image should be 2d!')\n",
    "    return Img_in\n",
    "\n",
    "def load_imgs_from_directory(load_directory,channel_name):\n",
    "    Img_collect = []\n",
    "    img_folders = glob(join(load_directory, \"*\", \"\"))\n",
    "    Img_file_list=[]\n",
    "\n",
    "    print('Image data loaded from ...\\n')\n",
    "    for sub_img_folder in img_folders:\n",
    "        Img_list = [f for f in listdir(sub_img_folder) if isfile(join(sub_img_folder, f)) & (f.endswith(\".tiff\") or f.endswith(\".tif\"))]\n",
    "        for Img_file in Img_list:\n",
    "            if channel_name.lower() in Img_file.lower():\n",
    "                Img_read = load_single_img(sub_img_folder + Img_file)\n",
    "                print(sub_img_folder + Img_file)\n",
    "                Img_file_list.append(Img_file)\n",
    "                Img_collect.append(Img_read)\n",
    "                break\n",
    "\n",
    "    print('\\n' + 'Image data loaded completed!')\n",
    "    if not Img_collect:\n",
    "        print('No such channels! Please check the channel name again!')\n",
    "        return\n",
    "\n",
    "    return Img_collect, Img_file_list, img_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unpack tiff stacks\n",
    "\n",
    "<font color='red'>input_folder </font> = The folder where the stacked tiff files are. You should be able to just copy and paste the whole .ome.tiff folder that the Bodenmiller pipeline creates after it has extracted the tiff files from the MCD files. This folde also contains the .csv panel files, copy those too! Any panorama files will also be in the same folder, but they won't be used here.\n",
    "\n",
    "<font color='red'>unstacked_output_folder </font> = Where the 'unstacked' tiff files will be stored. They will be unpacked into a single folder per ROI.\n",
    "\n",
    "<font color='red'>use_panel_files </font> = If you are using the Bodenmiller pipeline, leave this as True. It will use the .csv panel files for each ROI to properly label the unpacked channels with their metal tags and antigen targets, and will create a file called <font color='blue'>ROI_data.csv</font> which will store all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 56 channels were detected, and will be used if process_all_channels=True in the next step... \n",
      "\n",
      "['SMAa', 'GFAP', 'iba1', 'PanCytokeratin', 'S100B', 'EGFR', 'CD68', 'ki67', 'Neurocan', 'Fibronectin', 'SOX10', 'Brevican', 'CD44', 'Versican', 'PanLaminin', 'CD31', 'HIF1a', 'NeuN', 'CD109', 'OLIG2', 'CollagenIV', 'NG2', 'V0V1', 'GLUT1', 'Syndecan1', 'TCIRG1', 'pNFkB', 'TNC', 'CAIX', 'cMyc', 'pERK', 'TMEM119', 'SOX2', 'HepSul', 'CS56', 'MCT4', 'DNA1', 'DNA3', 'MHCI', 'CD14', 'CD16', 'CD11b', 'CDK4', 'YKL40', 'CD11c', 'CD24', 'VISTA', 'CD206', 'PTEN', 'Nestin', 'CD74', 'Met', 'P2YR12', 'CD163', 'HLADR', 'PDGFRa']\n"
     ]
    }
   ],
   "source": [
    "# The folder containing the \n",
    "input_folder = 'tiff_stacks' \n",
    "unstacked_output_folder = 'tiffs'\n",
    "use_panel_files=True\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "# Make output directories if they don't exist\n",
    "input_folder = Path(input_folder)\n",
    "output = Path(unstacked_output_folder)\n",
    "output.mkdir(exist_ok=True)\n",
    "\n",
    "# Setup a blank dataframe ready to add to\n",
    "if use_panel_files:\n",
    "    #roi_data = pd.DataFrame(columns=['panel_filename','channel_name','channel_label','filename','folder','fullstack_path'])\n",
    "    roi_data = pd.DataFrame(columns=['channel_name','channel_label'])\n",
    "# Get a list of all the .tiff files in the input directory\n",
    "tiff_files = list(input_folder.rglob('*.tiff'))\n",
    "\n",
    "for roi_count,i in enumerate(tiff_files):\n",
    "    \n",
    "    image = tp.imread(i)    \n",
    "    panel_filename = os.path.splitext(os.path.splitext(os.path.basename(i))[0])[0] + '.csv'\n",
    "    folder_name = os.path.splitext(os.path.basename(i))[0]\n",
    "\n",
    "    tiff_folder_name = os.path.splitext(os.path.basename(i))[0]    \n",
    "    output_dir = Path(unstacked_output_folder,tiff_folder_name)\n",
    "    output_dir.mkdir(exist_ok=True)        \n",
    "    \n",
    "    if use_panel_files:\n",
    "\n",
    "        panel_df = pd.read_csv(join(input_folder, panel_filename))\n",
    "        panel_df['fullstack_path'] = copy(str(i))       \n",
    "        panel_df['panel_filename']=panel_filename\n",
    "        panel_df['folder']=folder_name\n",
    "        roi_data = pd.concat([roi_data, panel_df], sort=True)\n",
    "    \n",
    "    for channel_count in range(image.shape[0]):\n",
    "        \n",
    "        if use_panel_files:\n",
    "            \n",
    "            panel_df['filename']=copy(str(channel_count)).zfill(2)+\"_\"+str(roi_count).zfill(2)+\"_\"+panel_df['channel_name']+\"_\"+panel_df['channel_label'].astype(str)+\".tiff\"\n",
    "            tp.imwrite(join(output_dir, panel_df.loc[channel_count,'filename']), image[channel_count])\n",
    "        else:\n",
    "            file_name=copy(str(channel_count)).zfill(2)+\"_\"+str(roi_count).zfill(2)+\".tiff\"\n",
    "            tp.imwrite(join(output_dir, file_name), image[channel_count])        \n",
    "        \n",
    "if use_panel_files:\n",
    "    roi_data.to_csv('ROI_data.csv')\n",
    "    \n",
    "    all_data_channels = roi_data.dropna().channel_label.unique().tolist()\n",
    "    n = len(all_data_channels)\n",
    "    print(f'The following {n} channels were detected, and will be used if process_all_channels=True in the next step... \\n')\n",
    "    print(roi_data.dropna().channel_label.unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run DeepSNF training and image denoising\n",
    "\n",
    "\n",
    "<font color='red'>raw_directory </font> = This should be the same as **'unstacked_output_folder'** above - where the unstacked images were stored, with each ROI being a folder containing all its images.\n",
    "\n",
    "<font color='red'>processed_output_dir </font> = The folder where the processed images will be stored. They will be in the same format as above - each ROI its own folder containing all its images.\n",
    "\n",
    "<font color='red'>process_all_channels </font> = If left as **True**, it will go through the channels identified above. Only works if you have panel files from the Bodenmiller pipeline, if not you will need to specify exactly which channels to process.\n",
    "\n",
    "<font color='red'>specific_channels </font> = If process_all_channels is False, you specify exactly which channels to process here, e.g. if you only want to process a couple.\n",
    "\n",
    "#### Deep SNF settings\n",
    "\n",
    "These all have accompanying explanations, and can mostly be left alone. Ones you may want to change include...\n",
    "\n",
    "<font color='red'>train_batch_size </font> You may need to reduce this to work on a GPU (e.g. to 32), or increase if you have a very good GPU setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data loaded from ...\n",
      "\n",
      "tiffs\\15_07_22_biomax_cell_ID_matrix_GBM_s0_a1_ac.ome\\01_00_Y89_SMAa.tiff\n",
      "tiffs\\15_07_22_biomax_cell_ID_matrix_GBM_s0_a2_ac.ome\\01_01_Y89_SMAa.tiff\n",
      "tiffs\\15_07_22_biomax_cell_ID_matrix_GBM_s0_a3_ac.ome\\01_02_Y89_SMAa.tiff\n",
      "tiffs\\15_07_22_biomax_cell_ID_matrix_GBM_s0_a4_ac.ome\\01_03_Y89_SMAa.tiff\n",
      "tiffs\\15_07_22_biomax_cell_ID_matrix_GBM_s0_a5_ac.ome\\01_04_Y89_SMAa.tiff\n",
      "tiffs\\15_07_22_biomax_cell_ID_matrix_GBM_s0_a6_ac.ome\\01_05_Y89_SMAa.tiff\n",
      "\n",
      "Image data loaded completed!\n",
      "The generated patches augmented.\n",
      "The generated patches shuffled.\n",
      "The shape of the generated training set is (13520, 64, 64).\n",
      "The range value to the corresponding model is 354.9147918701172.\n",
      "Input Channel Shape => (13520, 64, 64, 1)\n",
      "Number of Training Examples: 11492\n",
      "Number of Validation Examples: 2028\n",
      "Each training patch with shape of (64, 64) will mask 8 pixels.\n",
      "Training model...\n",
      "Epoch 1/50\n",
      "  4/180 [..............................] - ETA: 39:00 - loss: 0.6846"
     ]
    }
   ],
   "source": [
    "raw_directory = \"tiffs\" # change this directory to your Raw_image_directory.\n",
    "processed_output_dir = 'processed'\n",
    "process_all_channels = True\n",
    "specific_channels = []\n",
    "\n",
    "#################################################\n",
    "######  Deep SNF settings\n",
    "#################################################\n",
    "train_epoches = 50 # training epoches, which should be about 200 for a good training result. The default is 200.\n",
    "train_initial_lr = 1e-3 # inital learning rate. The default is 1e-3.\n",
    "train_batch_size = 64 # training batch size. For a GPU with smaller memory, it can be tuned smaller. The default is 256.\n",
    "pixel_mask_percent = 0.2 # percentage of the masked pixels in each patch. The default is 0.2.\n",
    "val_set_percent = 0.15 # percentage of validation set. The default is 0.15.\n",
    "loss_function = \"I_divergence\" # loss function used. The default is \"I_divergence\".\n",
    "loss_name = None # training and validation losses saved here, either .mat or .npz format. If not defined, the losses will not be saved.\n",
    "weights_save_directory = None # location where 'weights_name' and 'loss_name' saved.\n",
    "# If the value is None, the files will be saved in a sub-directory named \"trained_weights\" of  the current file folder.\n",
    "is_load_weights = False # Use the trained model directly. Will not read from saved one.\n",
    "lambda_HF = 3e-6 # HF regularization parameter\n",
    "\n",
    "\n",
    "# Create folders\n",
    "processed_output_dir = Path(processed_output_dir)\n",
    "processed_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "if process_all_channels:\n",
    "    channels = all_data_channels\n",
    "else:\n",
    "    channels = specific_channels\n",
    "\n",
    "    \n",
    "for channel_name in tqdm(channels):\n",
    "\n",
    "    if 'generated_patches' in globals():\n",
    "        del generated_patches    \n",
    "    \n",
    "    n_neighbours = 4 # Larger n enables removing more consecutive hot pixels. \n",
    "    n_iter = 3 # Iteration number for DIMR\n",
    "    window_size = 3 # Slide window size. For IMC images, window_size = 3 is fine.\n",
    "\n",
    "    DataGenerator = DeepSNF_DataGenerator(channel_name = channel_name, n_neighbours = n_neighbours, n_iter = n_iter, window_size = window_size)\n",
    "    generated_patches = DataGenerator.generate_patches_from_directory(load_directory = raw_directory)\n",
    "    print('The shape of the generated training set is ' + str(generated_patches.shape) + '.')\n",
    "\n",
    "    deepsnf = DeepSNF(train_epoches = train_epoches, \n",
    "                      train_learning_rate = train_initial_lr,\n",
    "                      train_batch_size = train_batch_size,\n",
    "                      mask_perc_pix = pixel_mask_percent,\n",
    "                      val_perc = val_set_percent,\n",
    "                      loss_func = loss_function,\n",
    "                      weights_name = \"weights_\"+str(channel_name)+\".hdf5\",\n",
    "                      loss_name = loss_name,\n",
    "                      weights_dir = weights_save_directory, \n",
    "                      is_load_weights = is_load_weights,\n",
    "                      lambda_HF = lambda_HF)\n",
    "\n",
    "    # Train the DeepSNF classifier \n",
    "    train_loss, val_loss = deepsnf.train(generated_patches)\n",
    "\n",
    "    # Load all images\n",
    "    Img_collect, Img_file_list, img_folders = load_imgs_from_directory(raw_directory, channel_name)\n",
    "\n",
    "    # Save resulting images\n",
    "    for i, img_file_name, folder in zip(Img_collect, Img_file_list, img_folders):\n",
    "        \n",
    "        #Perform both the hot pixel and shot noise \n",
    "        Img_DIMR_DeepSNF = deepsnf.perform_IMC_Denoise(i, n_neighbours = n_neighbours, n_iter = n_iter, window_size = window_size)\n",
    "\n",
    "        #Gets the ROI folder name from the path\n",
    "        roi_folder_name = Path(folder).parts[1]\n",
    "\n",
    "        #Makes sure the output folder name exists for this ROI\n",
    "        Path(join(processed_output_dir, roi_folder_name)).mkdir(exist_ok=True) \n",
    "        \n",
    "        #The output file is named the same as the input file\n",
    "        save_path = join(processed_output_dir, roi_folder_name, img_file_name)      \n",
    "\n",
    "        #Save the denoised file\n",
    "        tp.imsave(save_path,Img_DIMR_DeepSNF.astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Optional* - Assessing performance of denoise\n",
    "At this stage, you can check that the before and afters for the denoising for each channel\n",
    "\n",
    "<font color='red'>channels </font> List of channels you want to compare before and after denoising\n",
    "\n",
    "<font color='red'>colourmap </font> Colourmap for images\n",
    "\n",
    "<font color='red'>dpi </font> Resolution of generated images\n",
    "\n",
    "<font color='red'>save </font> Save images (or not). Will be saved as channel.png\n",
    "\n",
    "<font color='red'>do_all_channels </font> Will process for all channels with data as identified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=['Sma']\n",
    "colourmap ='jet'\n",
    "dpi=300\n",
    "save=True\n",
    "do_all_channels=False\n",
    "hide_images=False\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "if do_all_channels:\n",
    "    channel_list=all_data_channels\n",
    "else:\n",
    "    channel_list=channels\n",
    "\n",
    "\n",
    "for channel_name in channel_list:\n",
    "\n",
    "    raw_Img_collect, raw_Img_file_list, raw_img_folders = load_imgs_from_directory(raw_directory, channel_name)\n",
    "    pro_Img_collect, pro_Img_file_list, pro_img_folders = load_imgs_from_directory(processed_output_dir, channel_name)\n",
    "\n",
    "    fig, axs = plt.subplots(len(raw_Img_collect), 2, figsize=(10, 5*len(raw_Img_collect)), dpi=dpi)\n",
    "\n",
    "    count = 0\n",
    "    for r_img,p_img in zip(raw_Img_collect,pro_Img_collect):\n",
    "        im1= axs.flat[count].imshow(r_img, vmin = 0, vmax = 0.5*np.max(r_img), cmap = colourmap)\n",
    "        divider = make_axes_locatable(axs.flat[count])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im1, cax=cax, orientation='vertical')    \n",
    "        count=count+1\n",
    "\n",
    "        im2 = axs.flat[count].imshow(p_img, vmin = 0, vmax = 0.5*np.max(p_img), cmap = colourmap)\n",
    "        divider = make_axes_locatable(axs.flat[count])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im2, cax=cax, orientation='vertical')    \n",
    "        count=count+1 \n",
    "\n",
    "    fig.savefig(channel_name+'.png')\n",
    "    \n",
    "    if hide_images:\n",
    "        fig.set_visible(not fig.get_visible())\n",
    "        plt.draw()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reassemble TIFF stacks\n",
    "\n",
    "At this point, we want to reassemble the invidiual images back into stacks so we can put them back into the Bodenmiller pipeline, replacing the ones originally generated. You may want to keep backups of the unprocessed tiffs!\n",
    "\n",
    "<font color='red'> **By default, this pipeline will use all the processed image! If you only want to use some of the images, then manually assemble the individual TIFFs in the folders ready to be restacked**</font>\n",
    "\n",
    "\n",
    "<font color='red'>restack_input_folder </font> = This should be the same as **'processed_output_dir'** above - where the processed images were stored, with each ROI being a folder containing all its images.\n",
    "\n",
    "<font color='red'>restack_input_folder </font> = Where the processed and now restacked images should be place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify input and output folder\n",
    "restack_input_folder = 'processed'\n",
    "restacked_output_folder = 'processed_stacks'\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "\n",
    "# Make output directories if they don't exisit\n",
    "restack_input_folder = Path(restack_input_folder)\n",
    "output = Path(restacked_output_folder)\n",
    "output.mkdir(exist_ok=True)\n",
    "\n",
    "# Get a list of paths of ROI folder\n",
    "Img_folders = glob(join(restack_input_folder, \"*\", \"\"))\n",
    "\n",
    "print('Savings stacks...')\n",
    "for i in tqdm(Img_folders):\n",
    "    \n",
    "    tiff_files = list(Path(i).rglob('*.tiff'))\n",
    "    \n",
    "    image_stack=[]\n",
    "    \n",
    "    for file in tiff_files:\n",
    "        im = tp.imread(file).astype('float32')\n",
    "        image_stack.append(im)\n",
    "\n",
    "    image_stack = np.asarray(image_stack)\n",
    "        \n",
    "    save_path=join(restacked_output_folder, Path(i).parts[1]+\".tiff\")\n",
    "\n",
    "    tp.imsave(save_path,image_stack.astype('float32'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
