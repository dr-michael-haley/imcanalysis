{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up IMC-Denoise\n",
    "\n",
    "1. Follow the instructions under the **'Installation'** header from here: https://github.com/PENGLU-WashU/IMC_Denoise. In brief, you need to setup a new conda environment and install some packages with specific version numbers, and then clone and install the IMCDenoise package from Github.\n",
    "\n",
    "2. Run the following command in Anaconda prompt to install a couple of extra packages we will need in the new environment: **conda install tqdm pandas seaborn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and functions\n",
    "Run this to import all the relevant packages and functions. If there is an error here, you have not setup the environment properly. It may also be possible that your graphics card / GPU is not compatible with the script. It *should* all still run, but without acceleration from the GPU, it could be *incredibly* slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, abspath, exists\n",
    "from glob import glob\n",
    "import tifffile as tp\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tp\n",
    "from IMC_Denoise.IMC_Denoise_main.DIMR import DIMR\n",
    "from IMC_Denoise.IMC_Denoise_main.DeepSNF import DeepSNF\n",
    "from IMC_Denoise.DeepSNF_utils.DeepSNF_DataGenerator import DeepSNF_DataGenerator\n",
    "\n",
    "### These are adapted from functions from IMC_Denoise\n",
    "\n",
    "def load_single_img(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loading single image from directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : The image file name, must end with .tiff.\n",
    "        DESCRIPTION.\n",
    "    Returns\n",
    "    -------\n",
    "    Img_in : int or float\n",
    "        Loaded image data.\n",
    "    \"\"\"\n",
    "    if filename.endswith('.tiff') or filename.endswith('.tif'):\n",
    "        Img_in = tp.imread(filename).astype('float32')\n",
    "    else:\n",
    "        raise ValueError('Raw file should end with tiff or tif!')\n",
    "    if Img_in.ndim != 2:\n",
    "        raise ValueError('Single image should be 2d!')\n",
    "    return Img_in\n",
    "\n",
    "def load_imgs_from_directory(load_directory,channel_name,quiet=False):\n",
    "    Img_collect = []\n",
    "    img_folders = glob(join(load_directory, \"*\", \"\"))\n",
    "    Img_file_list=[]\n",
    "\n",
    "    if not quiet:\n",
    "        print('Image data loaded from ...\\n')\n",
    "    \n",
    "    for sub_img_folder in img_folders:\n",
    "        Img_list = [f for f in listdir(sub_img_folder) if isfile(join(sub_img_folder, f)) & (f.endswith(\".tiff\") or f.endswith(\".tif\"))]\n",
    "        for Img_file in Img_list:\n",
    "            if channel_name.lower() in Img_file.lower():\n",
    "                Img_read = load_single_img(sub_img_folder + Img_file)\n",
    "                \n",
    "                if not quiet:\n",
    "                    print(sub_img_folder + Img_file)\n",
    "                \n",
    "                Img_file_list.append(Img_file)\n",
    "                Img_collect.append(Img_read)\n",
    "                break\n",
    "\n",
    "    if not quiet:\n",
    "        print('\\n' + 'Image data loaded completed!')\n",
    "    \n",
    "    if not Img_collect:\n",
    "        print(f'No such channel as {channel_name}. Please check the channel name again!')\n",
    "        return\n",
    "\n",
    "    return Img_collect, Img_file_list, img_folders\n",
    "\n",
    "\n",
    "### This is for doing QC heatmaps and PCAs to look for outliers\n",
    "\n",
    "def qc_heatmap(directory='tiffs',\n",
    "                quantile=0.95,\n",
    "                save=True,\n",
    "                channels_list=channel_df['channel'].tolist(),\n",
    "                normalize=None,\n",
    "                figsize=(20,10),\n",
    "                dpi=200,             \n",
    "                save_dir='qc_images',\n",
    "                do_PCA=True,\n",
    "                annotate_PCA=True,\n",
    "              hide_figures=False):\n",
    "\n",
    "    # Create folder for saving\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create lists to save data into\n",
    "    channel_list=[]\n",
    "    roi_list=[]\n",
    "    img_max_list=[]\n",
    "    img_mean_list=[]\n",
    "    img_std_list=[]\n",
    "    img_q_list=[]\n",
    "\n",
    "    \n",
    "    print('Extracting data from images...\\n')\n",
    "    for channel in tqdm(channels_list):\n",
    "\n",
    "        Img_collect, Img_file_list, img_folders = load_imgs_from_directory(directory, channel, quiet=True)    \n",
    "\n",
    "        for img,img_f in zip(Img_collect,img_folders):\n",
    "            roi = Path(img_f).parts[1]\n",
    "            img_max=np.max(img)\n",
    "            img_mean=np.mean(img)\n",
    "            img_std=np.std(img)\n",
    "            img_q=np.quantile(img,quantile)\n",
    "\n",
    "            channel_list.append(copy(channel))\n",
    "            roi_list.append(copy(roi))\n",
    "            img_max_list.append(copy(img_max))\n",
    "            img_mean_list.append(copy(img_mean))\n",
    "            img_std_list.append(copy(img_std))\n",
    "            img_q_list.append(copy(img_q))\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(list(zip(channel_list, roi_list, img_max_list, img_mean_list, img_std_list, img_q_list)), columns=['channel','ROI','max','mean', 'std','quantile'])\n",
    "\n",
    "    print('Plotting results...\\n')\n",
    "\n",
    "    for i in ['max','mean','quantile', 'std','quantile']:\n",
    "        results_pivot = pd.pivot_table(results_df, index='channel',columns='ROI', values=i)\n",
    "\n",
    "        if normalize=='max':\n",
    "            results_pivot = results_pivot.div(results_pivot.max(axis=1), axis=0)\n",
    "            i = i+'_max_normalised'\n",
    "        elif normalize=='zscore':\n",
    "            results_pivot = results_pivot.apply(zscore, axis=0)\n",
    "            i = i+'_zscore'         \n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "        ax = sb.heatmap(results_pivot,xticklabels=True, yticklabels=True)\n",
    "        plt.title(i)\n",
    "\n",
    "        if save:\n",
    "            fig.savefig(join(save_dir,f'{directory}_{i}_heatmap.png'))\n",
    "        \n",
    "        if hide_figures:\n",
    "            plt.close()\n",
    "                        \n",
    "        if do_PCA:\n",
    "            import sklearn\n",
    "            from sklearn.decomposition import PCA\n",
    "\n",
    "            scaled_summary_data = sklearn.preprocessing.StandardScaler().fit_transform(results_pivot.T)\n",
    "\n",
    "            pca = PCA(n_components=2)\n",
    "            embedding = pca.fit_transform(scaled_summary_data)\n",
    "\n",
    "            #Create the graphs\n",
    "            fig, ax = plt.subplots(figsize=(10,10))\n",
    "            ax.scatter(\n",
    "                embedding[:, 0],\n",
    "                embedding[:, 1],\n",
    "                s=15)\n",
    "            \n",
    "            if annotate_PCA:\n",
    "                for loc, txt in zip(embedding,list(range(len(results_pivot.T.index)))):\n",
    "                    ax.annotate(txt, loc)  \n",
    "                pd.DataFrame(results_pivot.T.index).to_csv(join(save_dir,'roi_annotations.csv'))                    \n",
    "            \n",
    "            fig.gca().set_aspect('equal', 'datalim')\n",
    "            ax.set_xlabel('PCA1')\n",
    "            ax.set_ylabel('PCA2')\n",
    "            plt.title(i)\n",
    "            \n",
    "            if save:\n",
    "                plt.savefig(join(save_dir,f'{directory}_{i}_PCA.png'))\n",
    "                \n",
    "            if hide_figures:\n",
    "                plt.close()\n",
    "\n",
    "#### Deep SNF batch function            \n",
    "            \n",
    "def deep_SNF_batch(raw_directory = \"tiffs\",\n",
    "                   processed_output_dir = \"processed\",\n",
    "                   process_all_channels = True,\n",
    "                   specific_channels = [],\n",
    "                   patch_step_size=60,\n",
    "                   train_epoches = 50, # 50 gets good results in my experience.\n",
    "                    train_initial_lr = 1e-3, # inital learning rate. The default is 1e-3.\n",
    "                    train_batch_size = 128, # training batch size. For a GPU with smaller memory, it can be tuned smaller. The default is 256.\n",
    "                    pixel_mask_percent = 0.2, # percentage of the masked pixels in each patch. The default is 0.2.\n",
    "                    val_set_percent = 0.15, #percentage of validation set. The default is 0.15.\n",
    "                    loss_function = \"I_divergence\", # loss function used. The default is \"I_divergence\".\n",
    "                    loss_name = None, # training and validation losses saved here, either .mat or .npz format. If not defined, the losses will not be saved.\n",
    "                    weights_save_directory = None, # location where 'weights_name' and 'loss_name' saved.\n",
    "                    # If the value is None, the files will be saved in a sub-directory named \"trained_weights\" of  the current file folder.\n",
    "                    is_load_weights = False, # Use the trained model directly. Will not read from saved one.\n",
    "                    lambda_HF = 3e-6): # HF regularization parameter\n",
    "\n",
    "    # Error catching to make specific channels a list if just one channel given\n",
    "    if not isinstance(specific_channels, list):\n",
    "        specific_channels=[specific_channels]  \n",
    "\n",
    "    # Training settings\n",
    "    row_step=patch_step_size\n",
    "    col_step=patch_step_size \n",
    "\n",
    "    # Create folders\n",
    "    processed_output_dir = Path(processed_output_dir)\n",
    "    processed_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Error catching lists\n",
    "    error_channels=[]\n",
    "    completed_channels=[]\n",
    "\n",
    "    if process_all_channels:\n",
    "        channels = channel_df['channel'].tolist()\n",
    "    else:\n",
    "        channels = specific_channels\n",
    "\n",
    "\n",
    "    n = len(channels)\n",
    "    print(f'\\nPerforming denoising on the following {n} channels... \\n')\n",
    "    print(channels)\n",
    "    \n",
    "        \n",
    "    for channel_name in tqdm(channels):\n",
    "\n",
    "        try:\n",
    "\n",
    "            #if 'generated_patches' in globals():\n",
    "            #    del globals.generated_patches    \n",
    "\n",
    "            DataGenerator = DeepSNF_DataGenerator(channel_name = channel_name, \n",
    "                                                  n_neighbours = 4, # Larger n enables removing more consecutive hot pixels \n",
    "                                                  n_iter = 3, # Iteration number for DIMR\n",
    "                                                  window_size = 3, # Slide window size. For IMC images, window_size = 3 is fine.\n",
    "                                                  col_step=col_step,\n",
    "                                                  row_step=row_step)\n",
    "\n",
    "            generated_patches = DataGenerator.generate_patches_from_directory(load_directory = raw_directory)\n",
    "            print('The shape of the generated training set is ' + str(generated_patches.shape) + '.')\n",
    "\n",
    "            deepsnf = DeepSNF(train_epoches = train_epoches, \n",
    "                              train_learning_rate = train_initial_lr,\n",
    "                              train_batch_size = train_batch_size,\n",
    "                              mask_perc_pix = pixel_mask_percent,\n",
    "                              val_perc = val_set_percent,\n",
    "                              loss_func = loss_function,\n",
    "                              weights_name = \"weights_\"+str(channel_name)+\".hdf5\",\n",
    "                              loss_name = loss_name,\n",
    "                              weights_dir = weights_save_directory, \n",
    "                              is_load_weights = is_load_weights,\n",
    "                              lambda_HF = lambda_HF)\n",
    "\n",
    "            print('STARTING TRAINING...')\n",
    "            # Train the DeepSNF classifier \n",
    "            train_loss, val_loss = deepsnf.train(generated_patches)\n",
    "\n",
    "            # Load all images\n",
    "            Img_collect, Img_file_list, img_folders = load_imgs_from_directory(raw_directory, channel_name)\n",
    "\n",
    "            # Save resulting images\n",
    "            for i, img_file_name, folder in zip(Img_collect, Img_file_list, img_folders):\n",
    "\n",
    "                #Perform both the hot pixel and shot noise \n",
    "                Img_DIMR_DeepSNF = deepsnf.perform_IMC_Denoise(i, n_neighbours = n_neighbours, n_iter = n_iter, window_size = window_size)\n",
    "\n",
    "                #Gets the ROI folder name from the path\n",
    "                roi_folder_name = Path(folder).parts[1]\n",
    "\n",
    "                #Makes sure the output folder name exists for this ROI\n",
    "                Path(join(processed_output_dir, roi_folder_name)).mkdir(exist_ok=True) \n",
    "\n",
    "                #The output file is named the same as the input file\n",
    "                save_path = join(processed_output_dir, roi_folder_name, img_file_name)      \n",
    "\n",
    "                #Save the denoised file\n",
    "                tp.imsave(save_path,Img_DIMR_DeepSNF.astype('float32'))\n",
    "\n",
    "            completed_channels.append(channel_name)\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error in channel {channel_name}: {Exception}: {e}\")\n",
    "            error_channels.append(f\"{channel_name}: {Exception}: {e}\")\n",
    "\n",
    "    print(\"Successfull with channels:\")\n",
    "    print(completed_channels)\n",
    "    print(\"Channels with errors:\")\n",
    "    print(error_channels)\n",
    "    deep_SNF_batch.completed = completed_channels\n",
    "    deep_SNF_batch.errors = error_channels\n",
    "    \n",
    "# Side by side comparisson of before and after processing\n",
    "    \n",
    "def qc_check_side_by_side(channels=[],\n",
    "            colourmap ='jet',\n",
    "            dpi=200,\n",
    "            save=True,\n",
    "            save_dir='qc_images',\n",
    "            do_all_channels=True,\n",
    "            hide_images=True,\n",
    "            raw_directory='tiffs',\n",
    "            processed_output_dir='processed',\n",
    "            quiet=True):\n",
    "    \n",
    "    if not isinstance(channels, list):\n",
    "        channels=[channels]    \n",
    "    \n",
    "    # Create folders\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    if do_all_channels:\n",
    "        channel_list=all_data_channels\n",
    "    else:\n",
    "        channel_list=channels\n",
    "\n",
    "    # Error catching lists\n",
    "    error_channels=[]\n",
    "    completed_channels=[]    \n",
    "\n",
    "    for channel_name in channel_list:\n",
    "\n",
    "        try:\n",
    "\n",
    "            raw_Img_collect, raw_Img_file_list, raw_img_folders = load_imgs_from_directory(raw_directory, channel_name,quiet=quiet)\n",
    "            pro_Img_collect, pro_Img_file_list, pro_img_folders = load_imgs_from_directory(processed_output_dir, channel_name,quiet=quiet)\n",
    "\n",
    "            fig, axs = plt.subplots(len(raw_Img_collect), 2, figsize=(10, 5*len(raw_Img_collect)), dpi=dpi)\n",
    "\n",
    "            count = 0\n",
    "            for r_img,p_img in zip(raw_Img_collect,pro_Img_collect):\n",
    "                im1= axs.flat[count].imshow(r_img, vmin = 0, vmax = 0.5*np.max(r_img), cmap = colourmap)\n",
    "                divider = make_axes_locatable(axs.flat[count])\n",
    "                cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "                fig.colorbar(im1, cax=cax, orientation='vertical')    \n",
    "                count=count+1\n",
    "\n",
    "                im2 = axs.flat[count].imshow(p_img, vmin = 0, vmax = 0.5*np.max(p_img), cmap = colourmap)\n",
    "                divider = make_axes_locatable(axs.flat[count])\n",
    "                cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "                fig.colorbar(im2, cax=cax, orientation='vertical')    \n",
    "                count=count+1 \n",
    "\n",
    "            fig.savefig(join(save_dir, channel_name+'.png'))\n",
    "\n",
    "            if hide_images:\n",
    "                plt.close()\n",
    "\n",
    "            completed_channels.append(channel_name)\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error in channel {channel_name}: {Exception}: {e}\")\n",
    "            error_channels.append(f\"{channel_name}: {Exception}: {e}\")\n",
    "\n",
    "    print(\"Successfull with channels:\")\n",
    "    print(completed_channels)\n",
    "    print(\"Channels with errors:\")\n",
    "    print(error_channels)\n",
    "    qc_check_side_by_side.completed = completed_channels\n",
    "    qc_check_side_by_side.errors = error_channels        \n",
    "    \n",
    "    \n",
    "# Function to reassemble TIFF stacks\n",
    "\n",
    "def reassemble_stacks(restack_input_folder = 'tiffs',\n",
    "                      restacked_output_folder = 'tiffs_restacked'):\n",
    "\n",
    "    # Make output directories if they don't exisit\n",
    "    restack_input_folder = Path(restack_input_folder)\n",
    "    output = Path(restacked_output_folder)\n",
    "    output.mkdir(exist_ok=True)\n",
    "\n",
    "    # Get a list of paths of ROI folder\n",
    "    Img_folders = glob(join(restack_input_folder, \"*\", \"\"))\n",
    "\n",
    "    print('Savings stacks...')\n",
    "    for i in tqdm(Img_folders):\n",
    "\n",
    "        tiff_files = list(Path(i).rglob('*.tiff'))\n",
    "\n",
    "        image_stack=[]\n",
    "\n",
    "        for file in tiff_files:\n",
    "            im = tp.imread(str(file)).astype('float32')\n",
    "            image_stack.append(im)\n",
    "\n",
    "        image_stack = np.asarray(image_stack)\n",
    "\n",
    "        save_path=join(restacked_output_folder, Path(i).parts[1]+\".tiff\")\n",
    "\n",
    "        tp.imsave(save_path,image_stack.astype('float32'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Test\n",
    "\n",
    "This should return 'True' and the name of your GPU. If it doesn't, something has gone wrong in the setup of TensorFlow and/or CUDA that allows GPU-acceleration. Without it, the script will run *incredibly* low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU accceleration enabled\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.is_built_with_cuda()==True:\n",
    "    print('GPU accceleration enabled')\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "else:\n",
    "    print('GPU not found! Check TensorFlow and CUDA setup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unpack tiff stacks\n",
    "\n",
    "<font color='red'>input_folder </font> = The folder where the stacked tiff files are. You should be able to just copy and paste the whole .ome.tiff folder that the Bodenmiller pipeline creates after it has extracted the tiff files from the MCD files. This folde also contains the .csv panel files, copy those too! Any panorama files will also be in the same folder, but they won't be used here.\n",
    "\n",
    "<font color='red'>unstacked_output_folder </font> = Where the 'unstacked' tiff files will be stored. They will be unpacked into a single folder per ROI.\n",
    "\n",
    "<font color='red'>use_panel_files </font> = If you are using the Bodenmiller pipeline, leave this as True. It will use the .csv panel files for each ROI to properly label the unpacked channels with their metal tags and antigen targets, and will create a file called <font color='blue'>ROI_data.csv</font> which will store all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking ROIs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 95/95 [00:58<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 10 EMPTY channels were detected, and will be NOT be processed... \n",
      "\n",
      "['ArAr80', 'Ag107', 'Ag109', 'In113', 'Xe131', 'Xe134', 'Ba138', 'Gd157', 'Pt194', 'Pb208']\n",
      "\n",
      "The following 43 channels were detected, and will be used if process_all_channels=True in the next step... \n",
      "\n",
      "0                  Y89_SMA\n",
      "1               In115_Iba1\n",
      "2     La139_PanCytokeratin\n",
      "3              Pr141_S100B\n",
      "4               Nd142_MHCI\n",
      "5           Nd143_Vimentin\n",
      "6               Nd144_CD14\n",
      "7              Nd145_ICAM1\n",
      "8               Nd146_CD16\n",
      "9               Sm147_PD-1\n",
      "10            Nd148_SOX-10\n",
      "11             Sm149_CD11b\n",
      "12              Nd150_CD44\n",
      "13         Eu151_GranzymeB\n",
      "14              Sm152_CD45\n",
      "15              Eu153_CD31\n",
      "16             Sm154_CD11c\n",
      "17             Gd155_HIF1a\n",
      "18               Gd156_CD4\n",
      "19            Gd158_CD45RA\n",
      "20               Tb159_vWF\n",
      "21             Gd160_FOXP3\n",
      "22             Dy161_TIM-3\n",
      "23             Dy162_CD206\n",
      "24             Dy163_GLUT1\n",
      "25             Dy164_AGTR1\n",
      "26            Ho165_TCIRG1\n",
      "27              Er166_CD74\n",
      "28             Er167_PD-L1\n",
      "29             Er168_Ki-67\n",
      "30              Tm169_CAIX\n",
      "31               Er170_CD3\n",
      "32              Yb171_pERK\n",
      "33            Yb172_CX3CR1\n",
      "34            Yb173_CD45RO\n",
      "35            Yb174_HLA-DR\n",
      "36              Lu175_CD8a\n",
      "37              Yb176_MCT4\n",
      "38              Ir191_DNA1\n",
      "39              Ir193_DNA3\n",
      "40           Pt195_CD235ab\n",
      "41             Pt196_CD163\n",
      "42              Pt198_CD68\n",
      "Name: channel, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The folder containing the \n",
    "input_folder = 'tiff_stacks' \n",
    "unstacked_output_folder = 'tiffs'\n",
    "use_panel_files=True\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "# Make output directories if they don't exist\n",
    "input_folder = Path(input_folder)\n",
    "output = Path(unstacked_output_folder)\n",
    "output.mkdir(exist_ok=True)\n",
    "\n",
    "# Setup a blank dataframe ready to add to\n",
    "if use_panel_files:\n",
    "    #roi_data = pd.DataFrame(columns=['panel_filename','channel_name','channel_label','filename','folder','fullstack_path'])\n",
    "    roi_data = pd.DataFrame(columns=['channel_name','channel_label'])\n",
    "# Get a list of all the .tiff files in the input directory\n",
    "tiff_files = list(input_folder.rglob('*.tiff'))\n",
    "\n",
    "print('Unpacking ROIs...')\n",
    "for roi_count,i in enumerate(tqdm(tiff_files)):\n",
    "    \n",
    "    image = tp.imread(str(i))    \n",
    "\n",
    "    folder_name = os.path.splitext(os.path.basename(i))[0]\n",
    "\n",
    "    tiff_folder_name = os.path.splitext(os.path.basename(i))[0]    \n",
    "    output_dir = Path(unstacked_output_folder,tiff_folder_name)\n",
    "    output_dir.mkdir(exist_ok=True)        \n",
    "    \n",
    "    if use_panel_files:\n",
    "\n",
    "        panel_filename = os.path.splitext(os.path.splitext(os.path.basename(i))[0])[0] + '.csv'\n",
    "        panel_path = join(*i.parts[0:-1])\n",
    "        panel_df = pd.read_csv(join(panel_path, panel_filename))\n",
    "        panel_df['fullstack_path'] = copy(str(i))       \n",
    "        panel_df['panel_filename']=panel_filename\n",
    "        panel_df['folder']=folder_name\n",
    "        roi_data = pd.concat([roi_data, panel_df], sort=True)\n",
    "    \n",
    "    for channel_count in range(image.shape[0]):\n",
    "        \n",
    "        if use_panel_files:\n",
    "            \n",
    "            panel_df['filename']=copy(str(channel_count)).zfill(2)+\"_\"+str(roi_count).zfill(2)+\"_\"+panel_df['channel_name']+\"_\"+panel_df['channel_label'].astype(str)+\".tiff\"\n",
    "            tp.imwrite(join(output_dir, panel_df.loc[channel_count,'filename']), image[channel_count])\n",
    "        else:\n",
    "            file_name=copy(str(channel_count)).zfill(2)+\"_\"+str(roi_count).zfill(2)+\".tiff\"\n",
    "            tp.imwrite(join(output_dir, file_name), image[channel_count])        \n",
    "        \n",
    "if use_panel_files:\n",
    "    roi_data.to_csv('ROI_data.csv')       \n",
    "    all_data_channels = roi_data.dropna().channel_label.unique().tolist()\n",
    "    all_data_channel_names = roi_data.dropna().channel_name.unique().tolist()\n",
    "    channel_df = pd.DataFrame(list(zip(all_data_channel_names,all_data_channels)), columns = ['channel_name', 'channel_label'])\n",
    "    channel_df['channel']=channel_df['channel_name'] + \"_\" + channel_df['channel_label']\n",
    "    channel_df.to_csv('channels_list.csv')\n",
    "    \n",
    "    blank_channels = roi_data[roi_data.channel_label.isna()].channel_name.unique()\n",
    "    n = len(blank_channels)\n",
    "    print(f'The following {n} EMPTY channels were detected, and will be NOT be processed... \\n')\n",
    "    print(roi_data[roi_data.channel_label.isna()].channel_name.unique().tolist())\n",
    "    \n",
    "    n = len(all_data_channels)\n",
    "    print(f'\\nThe following {n} channels were detected, and will be used if process_all_channels=True in the next step... \\n')\n",
    "    print(channel_df['channel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. QC check on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run DeepSNF training and image denoising\n",
    "\n",
    "\n",
    "<font color='red'>raw_directory </font> = This should be the same as **'unstacked_output_folder'** above - where the unstacked images were stored, with each ROI being a folder containing all its images.\n",
    "\n",
    "<font color='red'>processed_output_dir </font> = The folder where the processed images will be stored. They will be in the same format as above - each ROI its own folder containing all its images.\n",
    "\n",
    "<font color='red'>process_all_channels </font> = If left as **True**, it will go through the channels identified above. Only works if you have panel files from the Bodenmiller pipeline, if not you will need to specify exactly which channels to process.\n",
    "\n",
    "<font color='red'>specific_channels </font> = If process_all_channels is False, you specify exactly which channels to process here, e.g. if you only want to process a couple.\n",
    "\n",
    "#### Deep SNF settings\n",
    "\n",
    "These all have accompanying explanations, and can mostly be left alone. Ones you may want to change include...\n",
    "\n",
    "<font color='red'>train_batch_size </font> If you are getting 'out of memory' errors you may need to reduce this to work on a GPU (e.g. to 32), or increase if you have a very good GPU setup.\n",
    "\n",
    "<font color='red'>patch_step_size </font> This is the frequency (in pixels) at which patches are taken from the dataset for training. If you are getting errors of being out of memory, usually because you have a huge dataset, increase this from its default of 60, to 100-150.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=['Eu151_GranzymeB',\n",
    "'Sm152_CD45',\n",
    "'Eu153_CD31',\n",
    "'Sm154_CD11c',\n",
    "'Gd155_HIF1a',\n",
    "'Gd156_CD4',\n",
    "'Gd158_CD45RA',\n",
    "'Tb159_vWF',\n",
    "'Gd160_FOXP3',\n",
    "'Dy161_TIM-3',\n",
    "'Dy162_CD206',\n",
    "'Dy163_GLUT1',\n",
    "'Dy164_AGTR1',\n",
    "'Ho165_TCIRG1',\n",
    "'Er166_CD74',\n",
    "'Er167_PD-L1',\n",
    "'Er168_Ki-67',\n",
    "'Tm169_CAIX',\n",
    "'Er170_CD3',\n",
    "'Yb171_pERK',\n",
    "'Yb172_CX3CR1',\n",
    "'Yb173_CD45RO',\n",
    "'Yb174_HLA-DR',\n",
    "'Lu175_CD8a',\n",
    "'Yb176_MCT4',\n",
    "'Ir191_DNA1',\n",
    "'Ir193_DNA3',\n",
    "'Pt195_CD235ab',\n",
    "'Pt196_CD163',\n",
    "'Pt198_CD68']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing denoising on the following 30 channels... \n",
      "\n",
      "['Eu151_GranzymeB', 'Sm152_CD45', 'Eu153_CD31', 'Sm154_CD11c', 'Gd155_HIF1a', 'Gd156_CD4', 'Gd158_CD45RA', 'Tb159_vWF', 'Gd160_FOXP3', 'Dy161_TIM-3', 'Dy162_CD206', 'Dy163_GLUT1', 'Dy164_AGTR1', 'Ho165_TCIRG1', 'Er166_CD74', 'Er167_PD-L1', 'Er168_Ki-67', 'Tm169_CAIX', 'Er170_CD3', 'Yb171_pERK', 'Yb172_CX3CR1', 'Yb173_CD45RO', 'Yb174_HLA-DR', 'Lu175_CD8a', 'Yb176_MCT4', 'Ir191_DNA1', 'Ir193_DNA3', 'Pt195_CD235ab', 'Pt196_CD163', 'Pt198_CD68']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data loaded from ...\n",
      "\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a10_ac.ome\\20_00_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a11_ac.ome\\20_01_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a12_ac.ome\\20_02_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a13_ac.ome\\20_03_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a14_ac.ome\\20_04_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a15_ac.ome\\20_05_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a16_ac.ome\\20_06_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a17_ac.ome\\20_07_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a18_ac.ome\\20_08_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a19_ac.ome\\20_09_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a1_ac.ome\\20_10_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a20_ac.ome\\20_11_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a22_ac.ome\\20_12_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a23_ac.ome\\20_13_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a24_ac.ome\\20_14_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a25_ac.ome\\20_15_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a26_ac.ome\\20_16_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a27_ac.ome\\20_17_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a2_ac.ome\\20_18_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a3_ac.ome\\20_19_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a4_ac.ome\\20_20_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a5_ac.ome\\20_21_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a6_ac.ome\\20_22_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a7_ac.ome\\20_23_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a8_ac.ome\\20_24_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA1 23.07.22_s0_a9_ac.ome\\20_25_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a10_ac.ome\\20_26_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a11_ac.ome\\20_27_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a12_ac.ome\\20_28_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a13_ac.ome\\20_29_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a14_ac.ome\\20_30_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a15_ac.ome\\20_31_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a16_ac.ome\\20_32_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a17_ac.ome\\20_33_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a18_ac.ome\\20_34_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a19_ac.ome\\20_35_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a1_ac.ome\\20_36_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a20_ac.ome\\20_37_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a21_ac.ome\\20_38_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a22_ac.ome\\20_39_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a23_ac.ome\\20_40_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a24_ac.ome\\20_41_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a25_ac.ome\\20_42_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a26_ac.ome\\20_43_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a27_ac.ome\\20_44_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a2_ac.ome\\20_45_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a3_ac.ome\\20_46_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a4_ac.ome\\20_47_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a5_ac.ome\\20_48_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a6_ac.ome\\20_49_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a7_ac.ome\\20_50_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a8_ac.ome\\20_51_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA2 25.05.22_s0_a9_ac.ome\\20_52_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a10_ac.ome\\20_53_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a11_ac.ome\\20_54_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a12_ac.ome\\20_55_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a13_ac.ome\\20_56_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a14_ac.ome\\20_57_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a15_ac.ome\\20_58_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a16_ac.ome\\20_59_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a17_ac.ome\\20_60_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a18_ac.ome\\20_61_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a19_ac.ome\\20_62_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a1_ac.ome\\20_63_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a20_ac.ome\\20_64_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a21_ac.ome\\20_65_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a22_ac.ome\\20_66_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a23_ac.ome\\20_67_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a24_ac.ome\\20_68_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a25_ac.ome\\20_69_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a26_ac.ome\\20_70_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a27_ac.ome\\20_71_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a28_ac.ome\\20_72_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a29_ac.ome\\20_73_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a2_ac.ome\\20_74_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a30_ac.ome\\20_75_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a31_ac.ome\\20_76_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a32_ac.ome\\20_77_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a33_ac.ome\\20_78_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a34_ac.ome\\20_79_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a3_ac.ome\\20_80_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a4_ac.ome\\20_81_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a5_ac.ome\\20_82_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a6_ac.ome\\20_83_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a7_ac.ome\\20_84_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA3 21.07.22_s0_a8_ac.ome\\20_85_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a1_ac.ome\\20_86_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a2_ac.ome\\20_87_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a3_ac.ome\\20_88_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a4_ac.ome\\20_89_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a5_ac.ome\\20_90_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a6_ac.ome\\20_91_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a7_ac.ome\\20_92_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a8_ac.ome\\20_93_Eu151_GranzymeB.tiff\n",
      "tiffs\\NF2 TMA4 27.07.22_s0_a9_ac.ome\\20_94_Eu151_GranzymeB.tiff\n",
      "\n",
      "Image data loaded completed!\n"
     ]
    }
   ],
   "source": [
    "deep_SNF_batch(specific_channels=channels,\n",
    "               patch_step_size=120,\n",
    "               train_batch_size=192,\n",
    "              process_all_channels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Side-by-side comparisson to check performance of denoising\n",
    "At this stage, you can check that the before and afters for the denoising for each channel\n",
    "\n",
    "<font color='red'>channels </font> List of channels you want to compare before and after denoising, by default it will do all of them\n",
    "\n",
    "<font color='red'>colourmap </font> Colourmap for images\n",
    "\n",
    "<font color='red'>dpi </font> Resolution of generated images\n",
    "\n",
    "<font color='red'>save </font> Save images (or not). Will be saved as channel.png\n",
    "\n",
    "<font color='red'>do_all_channels </font> Will process for all channels with data as identified above\n",
    "\n",
    "<font color='red'>hide_images </font> Will hide images (just saving them instead)\n",
    "\n",
    "By default, it will look for the raw and processed images in the directories specified above, but you can point to specific directories insteead (*raw_directory* and *processed_output_dir*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_check_side_by_side()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. QC check on processed data\n",
    "This will create heatmaps and PCAs that can be compared with those generated in step 2 on the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_heatmap(directory='processed', normalize='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manually recombine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reassemble TIFF stacks\n",
    "\n",
    "At this point, we want to reassemble the invidiual images back into stacks so we can put them back into the Bodenmiller pipeline, replacing the ones originally generated. You may want to keep backups of the unprocessed tiffs!\n",
    "\n",
    "<font color='red'> **By default, this pipeline will use all the processed image! If you only want to use some of the images, then manually assemble the individual TIFFs in the folders ready to be restacked**</font>\n",
    "\n",
    "\n",
    "<font color='red'>restack_input_folder </font> = This should be the same as processed output directory above - where the processed images were stored, with each ROI being a folder containing all its images. Default is **'tiffs'**. \n",
    "\n",
    "<font color='red'>restack_input_folder </font> = Where the processed and now restacked images should be place. Default is **'tiffs_restacked'**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reassemble_stacks(restack_input_folder = 'combined',\n",
    "                      restacked_output_folder = 'restacked')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
